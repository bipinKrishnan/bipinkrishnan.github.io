<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Getting Started in the World of Stable Diffusion | Bipin</title><meta name=keywords content="stable-diffusion,fastai-course"><meta name=description content="This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;From Deep learning foundations to Stable Diffusion&rsquo; by FastAI.
The things I am about to share in this post is based on the initial lessons of the course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.
Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?"><meta name=author content="Bipin Krishnan P"><link rel=canonical href=https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion.html><link crossorigin=anonymous href=../assets/css/stylesheet.80f99cabbb3de64440a55e13c3af93619eaf6e0c2d7f8662f08e6b314c5a1e46.css integrity="sha256-gPmcq7s95kRApV4Tw6+TYZ6vbgwtf4Zi8I5rMUxaHkY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://bipinkrishnan.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://bipinkrishnan.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://bipinkrishnan.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://bipinkrishnan.github.io/apple-touch-icon.png><link rel=mask-icon href=https://bipinkrishnan.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Getting Started in the World of Stable Diffusion"><meta property="og:description" content="This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;From Deep learning foundations to Stable Diffusion&rsquo; by FastAI.
The things I am about to share in this post is based on the initial lessons of the course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.
Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?"><meta property="og:type" content="article"><meta property="og:url" content="https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion.html"><meta property="og:image" content="https://bipinkrishnan.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-20T19:37:08+05:30"><meta property="article:modified_time" content="2022-10-20T19:37:08+05:30"><meta property="og:site_name" content="Bipin"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bipinkrishnan.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Getting Started in the World of Stable Diffusion"><meta name=twitter:description content="This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;From Deep learning foundations to Stable Diffusion&rsquo; by FastAI.
The things I am about to share in this post is based on the initial lessons of the course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.
Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Getting Started in the World of Stable Diffusion","item":"https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Getting Started in the World of Stable Diffusion","name":"Getting Started in the World of Stable Diffusion","description":"This is series of blog posts of me trying to share what I\u0026rsquo;ve explored in the course \u0026lsquo;From Deep learning foundations to Stable Diffusion\u0026rsquo; by FastAI.\nThe things I am about to share in this post is based on the initial lessons of the course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.\nLet\u0026rsquo;s jump right into the post with the question: \u0026lsquo;What is stable diffusion?","keywords":["stable-diffusion","fastai-course"],"articleBody":"This is series of blog posts of me trying to share what I’ve explored in the course ‘From Deep learning foundations to Stable Diffusion’ by FastAI.\nThe things I am about to share in this post is based on the initial lessons of the course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.\nLet’s jump right into the post with the question: ‘What is stable diffusion?’.\nWhat is stable diffusion? In simple words, stable diffusion is a deep learning model that can generate an image given a prompt like below:\nIt can also take an image as a starting point along with a prompt, to generate an image similar to the input image.For example, if the model is given a random drawing of wolf looking at the moon along with the prompt ‘wolf howling at the moon’, the model will generate a good looking image with similar setting as the input image:\nBut how does this whole thing work? First, let’s see how the model generated an awesome wolf image from a random sketch and a simple prompt. This generation process involves 3 different models:\nA model for converting the text prompt to embeddings. Openai’s CLIP(Contrastive Language-Image Pretraining) model is used for this purpose. A model for compressing the input image to a smaller dimension(this reduces the compute requirement for image generation). A Variational Autoencoder(VAE) model is used for this task. The last model generates the required image according to the prompt and input image. A U-Net model is used for this process. We will summarize each of the process above using simple diagrams:\nCLIP Embeddings As seen from the above figure, the prompt is converted to tokens using a tokenizer and these tokens are fed into the CLIP model to get the final text embeddings.\nEncoder part of VAE model The VAE model has an encoder as well as a decoder part. Given an image, the encoder part of the VAE compress the image to a low dimension tensor without much loss in information. This compressed form is called latents. The decoder is used to recover the original image from the latents. In the figure above only the encoder part is shown, we will talk more about decoder in the coming steps.\nU-Net model The U-Net model takes in the latents created by VAE as well as the text embedding created by the CLIP model as inputs. But before we pass in the latents, we add some noise to it, so that the U-Net model does not completely copy the image as it is.\nWith the noisy latents combined with text embeddings as inputs, the U-Net model tries to predict the noise present in the latents. We subtract this predicted noise from the latents to get the original latents. We repeat this process a specified number of times, say for example, 50 times. During each iteration, the amount of noise added to the latents is reduced.\nSo we start with a very noisy latent and gradually reduce the noise added.\nOnce the specified number of iterations is over, we get some latents that is ready to be decoded to an image. For this, we use the decoder part of the VAE model as shown below:\nDecoder part of VAE model This is simplified version of how stable diffusion model generates an image given a prompt and an initial image. Since these models work on latent space rather than the original image, these types of models are also called latent diffusion models.\nAs said earlier in the post, stable diffusion model can generate image even if we only provide a prompt describing the image we want. The above pipeline remains the same for this task also, but the only difference is that we don’t require the encoder part of the VAE model as we dont have any initial images to encode. So we just create a random tensor with the same size as the latents to feed to out U-Net model.\nLooking into the source code of huggingface diffusers Playing around with diffusers The diffusers library from huggingface already has implementations of the stable diffusion model and the library makes it easier to play around with these types of models.\nBefore you can play around with these models, you need to go here and accept the terms and conditions. Once you do that, you need to create a new token from here and copy it into the box that appears when you run the follwoing code in your jupyter notebook:\n!pip install huggingface-hub==0.10.1 from huggingface_hub import notebook_login notebook_login() Once everything is completed successfully, install the dependencies(as of writing this post, the latest version of diffusers and transformers are 0.6.0 and 4.23.1 respectively):\n!pip install -qq -U diffusers transformers Generating images with diffusers and stable diffusion is as simple as running these 3 lines of code(the code requires a GPU machine):\nfrom diffusers import StableDiffusionPipeline # set up the pipeline to generate images pipe = StableDiffusionPipeline.from_pretrained( 'CompVis/stable-diffusion-v1-4', ).to('cuda') prompt = \"a photograph of an astronaut riding a horse\" # generate an image for the above prompt pipe(prompt).images[0] If you wish to play around with diffusers pipeline for image generation, do check out their documentation here.\nDiving deeper into StableDiffusionPipeline Now let’s look into the source code and see what exactly is going on when we pass the prompt to our pipeline. The exact code that I’m discussing here can be found here.\nFirst, let’s learn more about the arguments in our pipeline: Other than the prompt we pass, there are a bunch of other arguments too. We can customize the image generation process by changing the default value of these arguments.\nLet’s check each of the arguments:\nprompt - this is the prompt that we pass to our pipeline as in pipe(prompt). height, width - these are the dimensions of the generated image. num_inference_steps - as we’ve discussed before, the unet model runs multiple times before generating the final latents, this argument defines this number. guidance_scale - guidance scale is the value that decides how close should our image be to the prompt. This is related to technique called Classifier-Free Guidance(CFD). negative_prompt - this is another argument that takes in a prompt as input. But as the name suggests, this is a negative prompt, thus, the model will try to generate images that doesn’t have any similarities with this prompt. As an example, if we provide ‘blue color’ as a negative prompt, the model will try to avoid including the color blue in the generated image. num_images_per_prompt - the number of images to generate for a given prompt. callback - this is mostly a callable function, that allows you to add some custom code into the pipeline. callback_steps - number of times the callback function will be called. The documentation page of diffusers has all these arguments listed with details included.\nNow let’s move to the improtant parts of the pipeline. The code below shows the text encoding part of the model: The first highlighted part shows the code for tokenizing our prompts using a tokenizer, pads it to the maximum length of the model and returns the tokens as pytorch tensors. The second highlighted part is where we pass these tokens to the CLIP model(text_encoder in the code) and gets the text embeddings.\nThe below code sets everything for classifier-free guidance we talked earlier: Classifier free guidance is done only if the guidance scale is greater than 1, the first highlighted part(line 239) checks this condition.\nFor doing classifier guidance, we need a new prompt called unconditional prompt which is an empty string(line 244) if no value is passed to negative_prompt, otherwise, the same negative prompt is used as unconditional prompt(line 251 \u0026 line 259).\nAfter deciding on the unconditional prompt, the prompts are tokenized(line 262) in the same manner as the normal prompts and encoded using the CLIP model(line 269).\nOnce we are ready with the unconditional embedding and the text embedding, we concatenate them and move on to the next step:\nThe concatenation part is done in the first highlighted part. Once we have the embedding part of our input ready, we need to prepare the second input, that is the latents. For that we just create a random tensor(line 295) with the same shape as the latents created by our VAE model from an image.\nWe have a scheduler which decides the amount of noise to be added to the latent at each iteration, so we give it the information regarding the maximum number of iterations(shown in line 302).\nAs said earlier we add noise to the latents before passing it to the U-Net model(shown in line 309).\nEverything is set, now it’s time to enter the generation step:\nIn the generation process, we iterate for the specified number of times(line 320). If we are doing classifier free guidance, we need to duplicate our latents(line 322) because we have an unconditional embedding and a text embedding. So in order to generate one image for each of these embeddings, we need separate latents for them.\nIn the next two lines of code, we scale the latents as required by the U-Net model and the model predicts the noise present in the input latents. Thus if we are doing classifier free guidance, the model will output two latents, otherwise, only one.\nFor classifier free guidance, we get the original noise to subtract from our latents from the follwoing formula(line 331):\npredicted_noise = noise_predicted_for_unconditional_embedding + guidance_scale*(noise_predicted_for_text_embedding - noise_predicted_for_unconditional_embedding) The above equation is where the guidance scale comes into play. It has a huge effect on the final noise subtracted from our latents.\nFinally, we pass the generated noise and the current latents to our scheduler to add noise for the next iteration(line 334). One thing to note here is that\nA new feature that was added recently is the callback function. If we pass any arguments to the callback argument, it gets executed inside the pipeline at line 338. For example, let’s create a simple function to pass into callback argument:\ndef sample_function(i, t, latents): # callback function expects the above 3 arguments print(i, t) Now if we pass in this function to the pipeline as pipe(prompt, callback=sample_function, callback_steps=3), then the function will be executed 3 times and each time it will print the values of i and t.\nAt last, we’ve reached the end of this source code deep dive, this is where the latents are coverted to image tensor using the decoder part of VAE model and then returns as a PIL image(shown in the highlighted part below).\nThe code that is in between the highlighted part(line 348 to 356) is used for checking the generated image for NSFW(Not Safe For Work) content. If this type of content is detected, the pipeline returns a black image as of the current version of the library(v0.6.0).\nConcluding thoughts This is just an overview of what’s happening inside diffusion models and the stable diffusion pipeline in diffusers.\nTo get an in-depth explanation on the topic and learn some cool tricks with stable diffusion, you should definitely checkout the FastAI course lectures and accompanying notebooks listed at the top of this blog post. If you have any queries or want to discuss something machine learning, you could reach out to me via twitter @bkrish_.\n","wordCount":"1898","inLanguage":"en","datePublished":"2022-10-20T19:37:08+05:30","dateModified":"2022-10-20T19:37:08+05:30","author":{"@type":"Person","name":"Bipin Krishnan P"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion.html"},"publisher":{"@type":"Organization","name":"Bipin","logo":{"@type":"ImageObject","url":"https://bipinkrishnan.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://bipinkrishnan.github.io accesskey=h title="Bipin (Alt + H)">Bipin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://bipinkrishnan.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://bipinkrishnan.github.io>Home</a></div><h1 class=post-title>Getting Started in the World of Stable Diffusion</h1><div class=post-meta><span title='2022-10-20 19:37:08 +0530 IST'>October 20, 2022</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Bipin Krishnan P</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-is-stable-diffusion>What is stable diffusion?</a></li><li><a href=#but-how-does-this-whole-thing-work>But how does this whole thing work?</a><ul><li><a href=#clip-embeddings>CLIP Embeddings</a></li><li><a href=#encoder-part-of-vae-model>Encoder part of VAE model</a></li><li><a href=#u-net-model>U-Net model</a></li><li><a href=#decoder-part-of-vae-model>Decoder part of VAE model</a></li></ul></li><li><a href=#looking-into-the-source-code-of-huggingface-diffusers>Looking into the source code of huggingface diffusers</a><ul><li><a href=#playing-around-with-diffusers>Playing around with diffusers</a></li><li><a href=#diving-deeper-into-stablediffusionpipeline>Diving deeper into <code>StableDiffusionPipeline</code></a></li></ul></li><li><a href=#concluding-thoughts>Concluding thoughts</a></li></ul></nav></div></details></div><div class=post-content><p><em>This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;<a href=https://www.fast.ai/posts/part2-2022.html>From Deep learning foundations to Stable Diffusion</a>&rsquo; by FastAI.</em></p><p>The things I am about to share in this post is based on the initial lessons of the course, which is publicly available <a href=https://www.fast.ai/posts/part2-2022-preview.html>here</a>. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this <a href=https://github.com/fastai/diffusion-nbs>Github repo</a>.</p><p>Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?&rsquo;.</p><h2 id=what-is-stable-diffusion>What is stable diffusion?<a hidden class=anchor aria-hidden=true href=#what-is-stable-diffusion>#</a></h2><p>In simple words, stable diffusion is a deep learning model that can generate an image given a prompt like below:</p><p><img loading=lazy src=../getting_started_stable_diffusion/sd-fox-intro.png alt="Image of fox generated by SD"></p><p>It can also take an image as a starting point along with a prompt, to generate an image similar to the input image.For example, if the model is given a random drawing of wolf looking at the moon along with the prompt &lsquo;wolf howling at the moon&rsquo;, the model will generate a good looking image with similar setting as the input image:</p><p><img loading=lazy src=../getting_started_stable_diffusion/sd-intro-wolf.png alt="Image of wolf generated by SD"></p><h2 id=but-how-does-this-whole-thing-work>But how does this whole thing work?<a hidden class=anchor aria-hidden=true href=#but-how-does-this-whole-thing-work>#</a></h2><p>First, let&rsquo;s see how the model generated an awesome wolf image from a random sketch and a simple prompt. This generation process involves 3 different models:</p><ol><li>A model for converting the text prompt to embeddings. Openai&rsquo;s CLIP(Contrastive Language-Image Pretraining) model is used for this purpose.</li><li>A model for compressing the input image to a smaller dimension(this reduces the compute requirement for image generation). A Variational Autoencoder(VAE) model is used for this task.</li><li>The last model generates the required image according to the prompt and input image. A U-Net model is used for this process.</li></ol><p>We will summarize each of the process above using simple diagrams:</p><h3 id=clip-embeddings>CLIP Embeddings<a hidden class=anchor aria-hidden=true href=#clip-embeddings>#</a></h3><p><img loading=lazy src=../getting_started_stable_diffusion/sd-text-encoder.png alt="SD text encoder model"></p><p>As seen from the above figure, the prompt is converted to tokens using a tokenizer and these tokens are fed into the CLIP model to get the final text embeddings.</p><h3 id=encoder-part-of-vae-model>Encoder part of VAE model<a hidden class=anchor aria-hidden=true href=#encoder-part-of-vae-model>#</a></h3><p><img loading=lazy src=../getting_started_stable_diffusion/sd-vae-encoder-arch.png alt="SD VAE encoder model"></p><p>The VAE model has an encoder as well as a decoder part. Given an image, the encoder part of the VAE compress the image to a low dimension tensor without much loss in information. This compressed form is called latents. The decoder is used to recover the original image from the latents. In the figure above only the encoder part is shown, we will talk more about decoder in the coming steps.</p><h3 id=u-net-model>U-Net model<a hidden class=anchor aria-hidden=true href=#u-net-model>#</a></h3><p><img loading=lazy src=../getting_started_stable_diffusion/sd-unet.png alt="UNET model"></p><p>The U-Net model takes in the latents created by VAE as well as the text embedding created by the CLIP model as inputs. But before we pass in the latents, we add some noise to it, so that the U-Net model does not completely copy the image as it is.</p><p>With the noisy latents combined with text embeddings as inputs, the U-Net model tries to predict the noise present in the latents. We subtract this predicted noise from the latents to get the original latents. We repeat this process a specified number of times, say for example, 50 times. During each iteration, the amount of noise added to the latents is reduced.</p><p>So we start with a very noisy latent and gradually reduce the noise added.</p><p>Once the specified number of iterations is over, we get some latents that is ready to be decoded to an image. For this, we use the decoder part of the VAE model as shown below:</p><h3 id=decoder-part-of-vae-model>Decoder part of VAE model<a hidden class=anchor aria-hidden=true href=#decoder-part-of-vae-model>#</a></h3><p><img loading=lazy src=../getting_started_stable_diffusion/sd-vae-decoder-arch.png alt="VAE decoder model"></p><p>This is simplified version of how stable diffusion model generates an image given a prompt and an initial image. Since these models work on latent space rather than the original image, these types of models are also called latent diffusion models.</p><p>As said earlier in the post, stable diffusion model can generate image even if we only provide a prompt describing the image we want. The above pipeline remains the same for this task also, but the only difference is that we don&rsquo;t require the encoder part of the VAE model as we dont have any initial images to encode. So we just create a random tensor with the same size as the latents to feed to out U-Net model.</p><h2 id=looking-into-the-source-code-of-huggingface-diffusers>Looking into the source code of huggingface diffusers<a hidden class=anchor aria-hidden=true href=#looking-into-the-source-code-of-huggingface-diffusers>#</a></h2><h3 id=playing-around-with-diffusers>Playing around with diffusers<a hidden class=anchor aria-hidden=true href=#playing-around-with-diffusers>#</a></h3><p>The diffusers library from huggingface already has implementations of the stable diffusion model and the library makes it easier to play around with these types of models.</p><p>Before you can play around with these models, you need to go <a href=https://huggingface.co/CompVis/stable-diffusion-v1-4>here</a> and accept the terms and conditions. Once you do that, you need to create a new token from <a href=https://huggingface.co/settings/tokens>here</a> and copy it into the box that appears when you run the follwoing code in your jupyter notebook:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>huggingface</span><span class=o>-</span><span class=n>hub</span><span class=o>==</span><span class=mf>0.10.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>huggingface_hub</span> <span class=kn>import</span> <span class=n>notebook_login</span>
</span></span><span class=line><span class=cl><span class=n>notebook_login</span><span class=p>()</span>
</span></span></code></pre></div><p>Once everything is completed successfully, install the dependencies(as of writing this post, the latest version of diffusers and transformers are <code>0.6.0</code> and <code>4.23.1</code> respectively):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>qq</span> <span class=o>-</span><span class=n>U</span> <span class=n>diffusers</span> <span class=n>transformers</span>
</span></span></code></pre></div><p>Generating images with diffusers and stable diffusion is as simple as running these 3 lines of code(the code requires a GPU machine):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># set up the pipeline to generate images</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;CompVis/stable-diffusion-v1-4&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;a photograph of an astronaut riding a horse&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># generate an image for the above prompt</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>If you wish to play around with diffusers pipeline for image generation, do check out their documentation <a href=https://huggingface.co/docs/diffusers/v0.6.0/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline>here</a>.</p><h3 id=diving-deeper-into-stablediffusionpipeline>Diving deeper into <code>StableDiffusionPipeline</code><a hidden class=anchor aria-hidden=true href=#diving-deeper-into-stablediffusionpipeline>#</a></h3><p>Now let&rsquo;s look into the source code and see what exactly is going on when we pass the prompt to our pipeline. The exact code that I&rsquo;m discussing here can be found <a href=https://github.com/huggingface/diffusers/blob/v0.6.0/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L122>here</a>.</p><p>First, let&rsquo;s learn more about the arguments in our pipeline:
<img loading=lazy src=../getting_started_stable_diffusion/diffusers-args.png alt="Diffiusers arguments"></p><p>Other than the prompt we pass, there are a bunch of other arguments too. We can customize the image generation process by changing the default value of these arguments.</p><p>Let&rsquo;s check each of the arguments:</p><ol><li><code>prompt</code> - this is the prompt that we pass to our pipeline as in <code>pipe(prompt)</code>.</li><li><code>height</code>, <code>width</code> - these are the dimensions of the generated image.</li><li><code>num_inference_steps</code> - as we&rsquo;ve discussed before, the unet model runs multiple times before generating the final latents, this argument defines this number.</li><li><code>guidance_scale</code> - guidance scale is the value that decides how close should our image be to the prompt. This is related to technique called <a href=https://benanne.github.io/2022/05/26/guidance.html>Classifier-Free Guidance(CFD)</a>.</li><li><code>negative_prompt</code> - this is another argument that takes in a prompt as input. But as the name suggests, this is a negative prompt, thus, the model will try to generate images that doesn&rsquo;t have any similarities with this prompt. As an example, if we provide &lsquo;blue color&rsquo; as a negative prompt, the model will try to avoid including the color blue in the generated image.</li><li><code>num_images_per_prompt</code> - the number of images to generate for a given prompt.</li><li><code>callback</code> - this is mostly a callable function, that allows you to add some custom code into the pipeline.</li><li><code>callback_steps</code> - number of times the callback function will be called.</li></ol><p>The <a href=https://huggingface.co/docs/diffusers/v0.6.0/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline.__call__>documentation page</a> of diffusers has all these arguments listed with details included.</p><p>Now let&rsquo;s move to the improtant parts of the pipeline. The code below shows the text encoding part of the model:
<img loading=lazy src=../getting_started_stable_diffusion/sd-text-encoder-code.png alt="Text encoder code"></p><p>The first highlighted part shows the code for tokenizing our prompts using a tokenizer, pads it to the maximum length of the model and returns the tokens as pytorch tensors. The second highlighted part is where we pass these tokens to the CLIP model(<code>text_encoder</code> in the code) and gets the text embeddings.</p><p>The below code sets everything for classifier-free guidance we talked earlier:
<img loading=lazy src=../getting_started_stable_diffusion/sd-cfd-code.png alt="setting the stage for classifier free guidance"></p><p>Classifier free guidance is done only if the guidance scale is greater than 1, the first highlighted part(line <code>239</code>) checks this condition.</p><p>For doing classifier guidance, we need a new prompt called unconditional prompt which is an empty string(line <code>244</code>) if no value is passed to <code>negative_prompt</code>, otherwise, the same negative prompt is used as unconditional prompt(line <code>251</code> & line <code>259</code>).</p><p>After deciding on the unconditional prompt, the prompts are tokenized(line <code>262</code>) in the same manner as the normal prompts and encoded using the CLIP model(line <code>269</code>).</p><p>Once we are ready with the unconditional embedding and the text embedding, we concatenate them and move on to the next step:</p><p><img loading=lazy src=../getting_started_stable_diffusion/sd-latents-code.png alt="Latents creation code"></p><p>The concatenation part is done in the first highlighted part. Once we have the embedding part of our input ready, we need to prepare the second input, that is the latents. For that we just create a random tensor(line <code>295</code>) with the same shape as the latents created by our VAE model from an image.</p><p>We have a scheduler which decides the amount of noise to be added to the latent at each iteration, so we give it the information regarding the maximum number of iterations(shown in line <code>302</code>).</p><p>As said earlier we add noise to the latents before passing it to the U-Net model(shown in line <code>309</code>).</p><p>Everything is set, now it&rsquo;s time to enter the generation step:</p><p><img loading=lazy src=../getting_started_stable_diffusion/sd-diffusion-loop.png alt="Diffusion loop"></p><p>In the generation process, we iterate for the specified number of times(line <code>320</code>). If we are doing classifier free guidance, we need to duplicate our latents(line <code>322</code>) because we have an unconditional embedding and a text embedding. So in order to generate one image for each of these embeddings, we need separate latents for them.</p><p>In the next two lines of code, we scale the latents as required by the U-Net model and the model predicts the noise present in the input latents. Thus if we are doing classifier free guidance, the model will output two latents, otherwise, only one.</p><p>For classifier free guidance, we get the original noise to subtract from our latents from the follwoing formula(line <code>331</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>predicted_noise</span> <span class=o>=</span> <span class=n>noise_predicted_for_unconditional_embedding</span> <span class=o>+</span> <span class=n>guidance_scale</span><span class=o>*</span><span class=p>(</span><span class=n>noise_predicted_for_text_embedding</span> <span class=o>-</span> <span class=n>noise_predicted_for_unconditional_embedding</span><span class=p>)</span>
</span></span></code></pre></div><p>The above equation is where the guidance scale comes into play. It has a huge effect on the final noise subtracted from our latents.</p><p>Finally, we pass the generated noise and the current latents to our scheduler to add noise for the next iteration(line <code>334</code>). One thing to note here is that</p><p>A new feature that was added recently is the callback function. If we pass any arguments to the <code>callback</code> argument, it gets executed inside the pipeline at line <code>338</code>. For example, let&rsquo;s create a simple function to pass into <code>callback</code> argument:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample_function</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>latents</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># callback function expects the above 3 arguments</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span></code></pre></div><p>Now if we pass in this function to the pipeline as <code>pipe(prompt, callback=sample_function, callback_steps=3)</code>, then the function will be executed 3 times and each time it will print the values of <code>i</code> and <code>t</code>.</p><p>At last, we&rsquo;ve reached the end of this source code deep dive, this is where the latents are coverted to image tensor using the decoder part of VAE model and then returns as a PIL image(shown in the highlighted part below).</p><p><img loading=lazy src=../getting_started_stable_diffusion/sd-vae-decode-code.png alt="Code for decoding the latents"></p><p>The code that is in between the highlighted part(line <code>348</code> to <code>356</code>) is used for checking the generated image for NSFW(Not Safe For Work) content. If this type of content is detected, the pipeline returns a black image as of the current version of the library(<code>v0.6.0</code>).</p><h2 id=concluding-thoughts>Concluding thoughts<a hidden class=anchor aria-hidden=true href=#concluding-thoughts>#</a></h2><p>This is just an overview of what&rsquo;s happening inside diffusion models and the stable diffusion pipeline in diffusers.</p><p>To get an in-depth explanation on the topic and learn some cool tricks with stable diffusion, you should definitely checkout the FastAI course lectures and accompanying notebooks listed at the top of this blog post. If you have any queries or want to discuss something machine learning, you could reach out to me via twitter <a href=https://twitter.com/@bkrish_>@bkrish_</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://bipinkrishnan.github.io/tags/stable-diffusion.html>stable-diffusion</a></li><li><a href=https://bipinkrishnan.github.io/tags/fastai-course.html>fastai-course</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on twitter" href="https://twitter.com/intent/tweet/?text=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html&hashtags=stable-diffusion%2cfastai-course"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html&title=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&summary=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&source=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html&title=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on whatsapp" href="https://api.whatsapp.com/send?text=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion%20-%20https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on telegram" href="https://telegram.me/share/url?text=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion.html"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://bipinkrishnan.github.io>Bipin</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>