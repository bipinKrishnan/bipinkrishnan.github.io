<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Getting Started in the World of Stable Diffusion | ðŸ“ŒPinnedNotes</title>
<meta name=keywords content="stable-diffusion,fastai-course,long-post">
<meta name=description content="This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;From Deep learning foundations to Stable Diffusion&rsquo; by FastAI.
Everything I share in this post is based on the initial lessons of the FastAI course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.
Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?">
<meta name=author content="Bipin Krishnan">
<link rel=canonical href=https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/>
<link crossorigin=anonymous href=/assets/css/stylesheet.80f99cabbb3de64440a55e13c3af93619eaf6e0c2d7f8662f08e6b314c5a1e46.css integrity="sha256-gPmcq7s95kRApV4Tw6+TYZ6vbgwtf4Zi8I5rMUxaHkY=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://bipinkrishnan.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://bipinkrishnan.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://bipinkrishnan.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://bipinkrishnan.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://bipinkrishnan.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script>
<meta property="og:title" content="Getting Started in the World of Stable Diffusion">
<meta property="og:description" content="This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;From Deep learning foundations to Stable Diffusion&rsquo; by FastAI.
Everything I share in this post is based on the initial lessons of the FastAI course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.
Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?">
<meta property="og:type" content="article">
<meta property="og:url" content="https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/"><meta property="og:image" content="https://bipinkrishnan.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-10-20T19:37:08+05:30">
<meta property="article:modified_time" content="2022-10-20T19:37:08+05:30"><meta property="og:site_name" content="ðŸ“ŒPinnedNotes">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://bipinkrishnan.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Getting Started in the World of Stable Diffusion">
<meta name=twitter:description content="This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;From Deep learning foundations to Stable Diffusion&rsquo; by FastAI.
Everything I share in this post is based on the initial lessons of the FastAI course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.
Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bipinkrishnan.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Getting Started in the World of Stable Diffusion","item":"https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Getting Started in the World of Stable Diffusion","name":"Getting Started in the World of Stable Diffusion","description":"This is series of blog posts of me trying to share what I\u0026rsquo;ve explored in the course \u0026lsquo;From Deep learning foundations to Stable Diffusion\u0026rsquo; by FastAI.\nEverything I share in this post is based on the initial lessons of the FastAI course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.\nLet\u0026rsquo;s jump right into the post with the question: \u0026lsquo;What is stable diffusion?","keywords":["stable-diffusion","fastai-course","long-post"],"articleBody":"This is series of blog posts of me trying to share what Iâ€™ve explored in the course â€˜From Deep learning foundations to Stable Diffusionâ€™ by FastAI.\nEverything I share in this post is based on the initial lessons of the FastAI course, which is publicly available here. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this Github repo.\nLetâ€™s jump right into the post with the question: â€˜What is stable diffusion?â€™.\nWhat is stable diffusion? In simple words, stable diffusion is a deep learning model that can generate an image given a prompt as shown in the figure below:\nIt can also take an image as a starting point along with a prompt, to generate an image similar to the input image. For example, if the model is given a random drawing of some animal looking at the moon along with the prompt â€˜wolf howling at the moonâ€™, the model will generate a good looking image with similar setting as the input image:\nBut how does this whole thing work? First, letâ€™s see how the model generated an awesome wolf image from a random sketch and a simple prompt. This generation process involves 3 different models:\n A model for converting the text prompt to embeddings. Openaiâ€™s CLIP(Contrastive Language-Image Pretraining) model is used for this purpose. A model for compressing the input image to a smaller dimension(this reduces the compute requirement for image generation). A Variational Autoencoder(VAE) model is used for this task. The last model generates the required image according to the prompt and input image. A U-Net model is used for this process.  We will summarize each of the process above using simple diagrams:\nCLIP Embeddings As seen in the above figure, CLIP tokenizer converts the prompt into tokens and feeds it into the CLIP model to get the text embeddings.\nEncoder part of VAE model The VAE model has an encoder as well as a decoder part. Given an image, the encoder part of the VAE compress the image to a low dimension tensor without much loss in information. This compressed form is called latents. The decoder is used to recover the original image from the latents. In the above figure, only the encoder part is shown. We will talk more about decoder in the coming steps.\nU-Net model The U-Net model takes in the latents created by VAE as well as the text embedding created by the CLIP model as inputs. But before we pass in the latents, we add some noise to it. Thus, the U-Net model does not copy the input image as it is.\nWith the noisy latents combined with text embeddings as inputs, the U-Net model tries to predict the noise present in the latents. We subtract this predicted noise from the latents to get the original latents. We repeat this process a specified number of times, say for example, 50 times. During each iteration, the amount of noise added to the latents is reduced. That is, for each iteration, the amount of noise added to the latents will be less than the previous iteration.\nIn short, we start with a very noisy latent and gradually reduce the noise added.\nDecoder part of VAE model Once the specified number of iterations is over, we get some latents that is ready to be decoded to an image. For this, we use the decoder part of the VAE model as shown below:\nThis is a simplified version of how stable diffusion model generates an image given a prompt and an initial image. Since these models work on latent space rather than the original image, these types of models are also called latent diffusion models.\nAs said earlier in this post, the stable diffusion model can generate images even if we only provide a prompt describing the image we want. The above pipeline remains the same for this task also, but the only difference is that we donâ€™t require the encoder part of the VAE model as we dont have any initial images to encode. So we just create a random tensor with the same size as the latents to feed to our U-Net model.\nLooking into the source code of huggingface diffusers Playing around with diffusers The diffusers library from huggingface already has implementations of the stable diffusion model and the library makes it easier to play around with these types of models.\nBefore you can play around with these models, you need to go here and accept the terms and conditions. Once you do that, you need to create a new token from here and copy it into the box that appears when you run the following code in your jupyter notebook:\n!pip install huggingface-hub==0.10.1 from huggingface_hub import notebook_login notebook_login() Once everything is completed successfully, install the dependencies(as of writing this post, the latest version of diffusers and transformers are 0.6.0 and 4.23.1 respectively):\n!pip install -qq -U diffusers transformers Generating images with diffusers and stable diffusion is as simple as running these 4 lines of code(the code requires a GPU machine):\nfrom diffusers import StableDiffusionPipeline # set up the pipeline to generate images pipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda') prompt = \"a photograph of an astronaut riding a horse\" # generate an image for the above prompt pipe(prompt).images[0] If you wish to play around with the diffusers pipeline for image generation, do check out their documentation here.\nDiving deeper into StableDiffusionPipeline Now letâ€™s look into the source code and see what exactly is going on when we pass the prompt to our pipeline. The exact code that Iâ€™m discussing here can be found here.\nFirst, letâ€™s learn more about the arguments passed to the stable diffusion pipeline: Other than the prompt, there are a bunch of other arguments too. We can customize the image generation process by changing the values of these arguments.\nLetâ€™s check each one them and see how they will help us:\n prompt - this is the prompt that we pass to our pipeline as in pipe(prompt). height, width - these are the dimensions of the generated image. num_inference_steps - as weâ€™ve discussed before, the U-Net model runs multiple times before generating the final latents. This is the denoising step. This argument defines the number of times to run the denoising step. guidance_scale - guidance scale is the value that decides how close our image should be to the prompt. This is related to a technique called Classifier-Free Guidance(CFD), which improves the quality of images generated. Higher values will generate images close to the prompt but the quality may not be the best. negative_prompt - this is another argument that takes in a prompt as input. But as the name suggests, this is a negative prompt. Thus, the model will try to generate images that doesnâ€™t have any similarities with this prompt. As an example, if we provide â€˜blue colorâ€™ as a negative prompt, the model will try to avoid including the color blue in the generated image. num_images_per_prompt - the number of images to generate for a given prompt. callback - this is mostly a callable function, that allows you to add some custom functionality into the pipeline. callback_steps - number of times the callback function is to be called.  The documentation page of diffusers has all these arguments listed with details included.\nNow letâ€™s move on to the improtant parts of the pipeline. First letâ€™s see how the text encoding part works: In the above image, the highlighted part at the top contains the code for tokenizing. The prompt is tokenized, padded to the maximum length of the model and converted to pytorch tensors using a tokenizer. The second highlighted part is where we pass these tokens to the CLIP model(text_encoder in the code) to get the text embeddings.\nNow letâ€™s look into some code for doing classifier-free guidance: Classifier free guidance is done only if the guidance scale is greater than 1, the highlighted part at the top(line 239) checks this condition.\nFor doing classifier guidance, we need a new prompt called unconditional prompt which is an empty string(line 244) if no value is passed to negative_prompt, otherwise, the same negative prompt is used as unconditional prompt(line 251 \u0026 line 259).\nAfter deciding on the unconditional prompt, the prompts are tokenized(line 262) in the same manner as the normal prompts and encoded using the CLIP model(line 269).\nOnce we are ready with the unconditional embedding and the text embedding, we concatenate them and move on to the next step:\nThe concatenation part is done in the first highlighted part. Once we have the embedding part of our input ready, we need to prepare the second input, that is the latents. For that we just create a random tensor(line 295) with the same shape as the latents created by our VAE model for an image.\nWe have a scheduler which decides the amount of noise to be added to the latent at each iteration, so we give it the information regarding the maximum number of iterations possible(shown in line 302).\nAs said earlier we add noise to the latents before passing it to the U-Net model(shown in line 309).\nEverything is set, now itâ€™s time to enter the generation step:\nIn the generation process, we iterate till the maximum number is reached(line 320). If we are doing classifier free guidance, we need to duplicate our latents(line 322) because we have an unconditional embedding and a text embedding. So in order to generate one image for each of these embeddings, we need separate latents for them.\nIn the next two lines of code, we scale the latents as required by the U-Net model and the model predicts the noise present in the input latents. Thus if we are doing classifier free guidance, the model will output two latents, otherwise, only one.\nFor classifier free guidance, we get the predicted noise using the follwoing formula(line 331):\npredicted_noise = noise_predicted_for_unconditional_embedding + guidance_scale*(noise_predicted_for_text_embedding - noise_predicted_for_unconditional_embedding) The above equation is where the guidance scale comes into play. It has a huge effect on the predicted noise subtracted from our latents.\nFinally, we pass the predicted noise and the current latents to our scheduler to prepare the latents for the next iteration(line 334).\nA new feature that was added recently is the callback function. If we pass any arguments to the callback argument, it gets executed inside the pipeline at line 338. For example, letâ€™s create a simple function to pass into callback argument:\ndef sample_function(i, t, latents): # callback function should expect the above 3 arguments print(i, t) Now if we pass this function to the pipeline as pipe(prompt, callback=sample_function, callback_steps=3), then the function will be executed 3 times and each time it will print the values of i and t.\nAt last, weâ€™ve reached the end of the pipeline source code. This is where the latents are coverted to image format using the decoder part of VAE model and finally returned as a PIL image, as shown in the highlighted part below:\nThe code that is in between the highlighted part(line 348 to 356) checks the generated image for NSFW(Not Safe For Work) content. If this type of content is detected, the pipeline returns a black image(as of the current version of the library - v0.6.0).\nConcluding thoughts This is just an overview of whatâ€™s happening inside diffusion models and the stable diffusion pipeline in diffusers.\nThe FastAI course lectures and accompanying notebooks listed at the top of this blog post has some great explanations on the topics discussed so far. The notebooks also contain code to do some cool tricks using stable diffusion.\nIf you have any queries or want to discuss something machine learning, you could reach out to me via twitter @bkrish_.\n","wordCount":"1958","inLanguage":"en","datePublished":"2022-10-20T19:37:08+05:30","dateModified":"2022-10-20T19:37:08+05:30","author":{"@type":"Person","name":"Bipin Krishnan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/"},"publisher":{"@type":"Organization","name":"ðŸ“ŒPinnedNotes","logo":{"@type":"ImageObject","url":"https://bipinkrishnan.github.io/favicon.ico"}}}</script>
</head>
<body class=dark id=top>
<script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://bipinkrishnan.github.io accesskey=h title="ðŸ“ŒPinnedNotes (Alt + H)">ðŸ“ŒPinnedNotes</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://bipinkrishnan.github.io/tags/long-post/ title="Long Posts">
<span>Long Posts</span>
</a>
</li>
<li>
<a href=https://bipinkrishnan.github.io/tags/quick-explainer/ title=Explainers>
<span>Explainers</span>
</a>
</li>
<li>
<a href=https://bipinkrishnan.github.io/archives/ title=Archives>
<span>Archives</span>
</a>
</li>
<li>
<a href=https://bipinkrishnan.github.io/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://bipinkrishnan.github.io>Home</a>&nbsp;Â»&nbsp;<a href=https://bipinkrishnan.github.io/posts/>Posts</a></div>
<h1 class=post-title>
Getting Started in the World of Stable Diffusion
</h1>
<div class=post-meta><span title="2022-10-20 19:37:08 +0530 +0530">October 20, 2022</span>&nbsp;Â·&nbsp;10 min&nbsp;Â·&nbsp;Bipin Krishnan
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#what-is-stable-diffusion>What is stable diffusion?</a></li>
<li><a href=#but-how-does-this-whole-thing-work>But how does this whole thing work?</a>
<ul>
<li><a href=#clip-embeddings>CLIP Embeddings</a></li>
<li><a href=#encoder-part-of-vae-model>Encoder part of VAE model</a></li>
<li><a href=#u-net-model>U-Net model</a></li>
<li><a href=#decoder-part-of-vae-model>Decoder part of VAE model</a></li>
</ul>
</li>
<li><a href=#looking-into-the-source-code-of-huggingface-diffusers>Looking into the source code of huggingface diffusers</a>
<ul>
<li><a href=#playing-around-with-diffusers>Playing around with diffusers</a></li>
<li><a href=#diving-deeper-into-stablediffusionpipeline>Diving deeper into <code>StableDiffusionPipeline</code></a></li>
</ul>
</li>
<li><a href=#concluding-thoughts>Concluding thoughts</a></li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><p><em>This is series of blog posts of me trying to share what I&rsquo;ve explored in the course &lsquo;<a href=https://www.fast.ai/posts/part2-2022.html>From Deep learning foundations to Stable Diffusion</a>&rsquo; by FastAI.</em></p>
<p>Everything I share in this post is based on the initial lessons of the FastAI course, which is publicly available <a href=https://www.fast.ai/posts/part2-2022-preview.html>here</a>. The lessons also have some accompanying jupyter notebooks on stable diffusion, which can be found in this <a href=https://github.com/fastai/diffusion-nbs>Github repo</a>.</p>
<p>Let&rsquo;s jump right into the post with the question: &lsquo;What is stable diffusion?&rsquo;.</p>
<h2 id=what-is-stable-diffusion>What is stable diffusion?<a hidden class=anchor aria-hidden=true href=#what-is-stable-diffusion>#</a></h2>
<p>In simple words, stable diffusion is a deep learning model that can generate an image given a prompt as shown in the figure below:</p>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-fox-intro.png alt="Image of fox generated by SD">
</p>
<p>It can also take an image as a starting point along with a prompt, to generate an image similar to the input image. For example, if the model is given a random drawing of some animal looking at the moon along with the prompt &lsquo;wolf howling at the moon&rsquo;, the model will generate a good looking image with similar setting as the input image:</p>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-intro-wolf.png alt="Image of wolf generated by SD">
</p>
<h2 id=but-how-does-this-whole-thing-work>But how does this whole thing work?<a hidden class=anchor aria-hidden=true href=#but-how-does-this-whole-thing-work>#</a></h2>
<p>First, let&rsquo;s see how the model generated an awesome wolf image from a random sketch and a simple prompt. This generation process involves 3 different models:</p>
<ol>
<li>A model for converting the text prompt to embeddings. Openai&rsquo;s CLIP(Contrastive Language-Image Pretraining) model is used for this purpose.</li>
<li>A model for compressing the input image to a smaller dimension(this reduces the compute requirement for image generation). A Variational Autoencoder(VAE) model is used for this task.</li>
<li>The last model generates the required image according to the prompt and input image. A U-Net model is used for this process.</li>
</ol>
<p>We will summarize each of the process above using simple diagrams:</p>
<h3 id=clip-embeddings>CLIP Embeddings<a hidden class=anchor aria-hidden=true href=#clip-embeddings>#</a></h3>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-text-encoder.png alt="SD text encoder model">
</p>
<p>As seen in the above figure, CLIP tokenizer converts the prompt into tokens and feeds it into the CLIP model to get the text embeddings.</p>
<h3 id=encoder-part-of-vae-model>Encoder part of VAE model<a hidden class=anchor aria-hidden=true href=#encoder-part-of-vae-model>#</a></h3>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-vae-encoder-arch.png alt="SD VAE encoder model">
</p>
<p>The VAE model has an encoder as well as a decoder part. Given an image, the encoder part of the VAE compress the image to a low dimension tensor without much loss in information. This compressed form is called latents. The decoder is used to recover the original image from the latents. In the above figure, only the encoder part is shown. We will talk more about decoder in the coming steps.</p>
<h3 id=u-net-model>U-Net model<a hidden class=anchor aria-hidden=true href=#u-net-model>#</a></h3>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-unet.png alt="UNET model">
</p>
<p>The U-Net model takes in the latents created by VAE as well as the text embedding created by the CLIP model as inputs. But before we pass in the latents, we add some noise to it. Thus, the U-Net model does not copy the input image as it is.</p>
<p>With the noisy latents combined with text embeddings as inputs, the U-Net model tries to predict the noise present in the latents. We subtract this predicted noise from the latents to get the original latents. We repeat this process a specified number of times, say for example, 50 times. During each iteration, the amount of noise added to the latents is reduced. That is, for each iteration, the amount of noise added to the latents will be less than the previous iteration.</p>
<p>In short, we start with a very noisy latent and gradually reduce the noise added.</p>
<h3 id=decoder-part-of-vae-model>Decoder part of VAE model<a hidden class=anchor aria-hidden=true href=#decoder-part-of-vae-model>#</a></h3>
<p>Once the specified number of iterations is over, we get some latents that is ready to be decoded to an image. For this, we use the decoder part of the VAE model as shown below:</p>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-vae-decoder-arch.png alt="VAE decoder model">
</p>
<p>This is a simplified version of how stable diffusion model generates an image given a prompt and an initial image. Since these models work on latent space rather than the original image, these types of models are also called latent diffusion models.</p>
<p>As said earlier in this post, the stable diffusion model can generate images even if we only provide a prompt describing the image we want. The above pipeline remains the same for this task also, but the only difference is that we don&rsquo;t require the encoder part of the VAE model as we dont have any initial images to encode. So we just create a random tensor with the same size as the latents to feed to our U-Net model.</p>
<h2 id=looking-into-the-source-code-of-huggingface-diffusers>Looking into the source code of huggingface diffusers<a hidden class=anchor aria-hidden=true href=#looking-into-the-source-code-of-huggingface-diffusers>#</a></h2>
<h3 id=playing-around-with-diffusers>Playing around with diffusers<a hidden class=anchor aria-hidden=true href=#playing-around-with-diffusers>#</a></h3>
<p>The <a href=https://github.com/huggingface/diffusers>diffusers library</a> from huggingface already has implementations of the stable diffusion model and the library makes it easier to play around with these types of models.</p>
<p>Before you can play around with these models, you need to go <a href=https://huggingface.co/CompVis/stable-diffusion-v1-4>here</a> and accept the terms and conditions. Once you do that, you need to create a new token from <a href=https://huggingface.co/settings/tokens>here</a> and copy it into the box that appears when you run the following code in your jupyter notebook:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>huggingface</span><span class=o>-</span><span class=n>hub</span><span class=o>==</span><span class=mf>0.10.1</span>

<span class=kn>from</span> <span class=nn>huggingface_hub</span> <span class=kn>import</span> <span class=n>notebook_login</span>
<span class=n>notebook_login</span><span class=p>()</span>
</code></pre></div><p>Once everything is completed successfully, install the dependencies(as of writing this post, the latest version of diffusers and transformers are <code>0.6.0</code> and <code>4.23.1</code> respectively):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>qq</span> <span class=o>-</span><span class=n>U</span> <span class=n>diffusers</span> <span class=n>transformers</span>
</code></pre></div><p>Generating images with diffusers and stable diffusion is as simple as running these 4 lines of code(the code requires a GPU machine):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionPipeline</span>

<span class=c1># set up the pipeline to generate images</span>
<span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;CompVis/stable-diffusion-v1-4&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>

<span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;a photograph of an astronaut riding a horse&#34;</span>
<span class=c1># generate an image for the above prompt</span>
<span class=n>pipe</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</code></pre></div><p>If you wish to play around with the diffusers pipeline for image generation, do check out their documentation <a href=https://huggingface.co/docs/diffusers/v0.6.0/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline>here</a>.</p>
<h3 id=diving-deeper-into-stablediffusionpipeline>Diving deeper into <code>StableDiffusionPipeline</code><a hidden class=anchor aria-hidden=true href=#diving-deeper-into-stablediffusionpipeline>#</a></h3>
<p>Now let&rsquo;s look into the source code and see what exactly is going on when we pass the prompt to our pipeline. The exact code that I&rsquo;m discussing here can be found <a href=https://github.com/huggingface/diffusers/blob/v0.6.0/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L122>here</a>.</p>
<p>First, let&rsquo;s learn more about the arguments passed to the stable diffusion pipeline:
<img loading=lazy src=/getting_started_stable_diffusion/diffusers-args.png alt="Diffiusers arguments">
</p>
<p>Other than the prompt, there are a bunch of other arguments too. We can customize the image generation process by changing the values of these arguments.</p>
<p>Let&rsquo;s check each one them and see how they will help us:</p>
<ol>
<li><code>prompt</code> - this is the prompt that we pass to our pipeline as in <code>pipe(prompt)</code>.</li>
<li><code>height</code>, <code>width</code> - these are the dimensions of the generated image.</li>
<li><code>num_inference_steps</code> - as we&rsquo;ve discussed before, the U-Net model runs multiple times before generating the final latents. This is the denoising step. This argument defines the number of times to run the denoising step.</li>
<li><code>guidance_scale</code> - guidance scale is the value that decides how close our image should be to the prompt. This is related to a technique called <a href=https://benanne.github.io/2022/05/26/guidance.html>Classifier-Free Guidance(CFD)</a>, which improves the quality of images generated. Higher values will generate images close to the prompt but the quality may not be the best.</li>
<li><code>negative_prompt</code> - this is another argument that takes in a prompt as input. But as the name suggests, this is a negative prompt. Thus, the model will try to generate images that doesn&rsquo;t have any similarities with this prompt. As an example, if we provide &lsquo;blue color&rsquo; as a negative prompt, the model will try to avoid including the color blue in the generated image.</li>
<li><code>num_images_per_prompt</code> - the number of images to generate for a given prompt.</li>
<li><code>callback</code> - this is mostly a callable function, that allows you to add some custom functionality into the pipeline.</li>
<li><code>callback_steps</code> - number of times the callback function is to be called.</li>
</ol>
<p>The <a href=https://huggingface.co/docs/diffusers/v0.6.0/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline.__call__>documentation page</a> of diffusers has all these arguments listed with details included.</p>
<p>Now let&rsquo;s move on to the improtant parts of the pipeline. First let&rsquo;s see how the text encoding part works:
<img loading=lazy src=/getting_started_stable_diffusion/sd-text-encoder-code.png alt="Text encoder code">
</p>
<p>In the above image, the highlighted part at the top contains the code for tokenizing. The prompt is tokenized, padded to the maximum length of the model and converted to pytorch tensors using a tokenizer. The second highlighted part is where we pass these tokens to the CLIP model(<code>text_encoder</code> in the code) to get the text embeddings.</p>
<p>Now let&rsquo;s look into some code for doing classifier-free guidance:
<img loading=lazy src=/getting_started_stable_diffusion/sd-cfd-code.png alt="setting the stage for classifier free guidance">
</p>
<p>Classifier free guidance is done only if the guidance scale is greater than 1, the highlighted part at the top(line <code>239</code>) checks this condition.</p>
<p>For doing classifier guidance, we need a new prompt called unconditional prompt which is an empty string(line <code>244</code>) if no value is passed to <code>negative_prompt</code>, otherwise, the same negative prompt is used as unconditional prompt(line <code>251</code> & line <code>259</code>).</p>
<p>After deciding on the unconditional prompt, the prompts are tokenized(line <code>262</code>) in the same manner as the normal prompts and encoded using the CLIP model(line <code>269</code>).</p>
<p>Once we are ready with the unconditional embedding and the text embedding, we concatenate them and move on to the next step:</p>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-latents-code.png alt="Latents creation code">
</p>
<p>The concatenation part is done in the first highlighted part. Once we have the embedding part of our input ready, we need to prepare the second input, that is the latents. For that we just create a random tensor(line <code>295</code>) with the same shape as the latents created by our VAE model for an image.</p>
<p>We have a scheduler which decides the amount of noise to be added to the latent at each iteration, so we give it the information regarding the maximum number of iterations possible(shown in line <code>302</code>).</p>
<p>As said earlier we add noise to the latents before passing it to the U-Net model(shown in line <code>309</code>).</p>
<p>Everything is set, now it&rsquo;s time to enter the generation step:</p>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-diffusion-loop.png alt="Diffusion loop">
</p>
<p>In the generation process, we iterate till the maximum number is reached(line <code>320</code>). If we are doing classifier free guidance, we need to duplicate our latents(line <code>322</code>) because we have an unconditional embedding and a text embedding. So in order to generate one image for each of these embeddings, we need separate latents for them.</p>
<p>In the next two lines of code, we scale the latents as required by the U-Net model and the model predicts the noise present in the input latents. Thus if we are doing classifier free guidance, the model will output two latents, otherwise, only one.</p>
<p>For classifier free guidance, we get the predicted noise using the follwoing formula(line <code>331</code>):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>predicted_noise</span> <span class=o>=</span> <span class=n>noise_predicted_for_unconditional_embedding</span> <span class=o>+</span> <span class=n>guidance_scale</span><span class=o>*</span><span class=p>(</span><span class=n>noise_predicted_for_text_embedding</span> <span class=o>-</span> <span class=n>noise_predicted_for_unconditional_embedding</span><span class=p>)</span>
</code></pre></div><p>The above equation is where the guidance scale comes into play. It has a huge effect on the predicted noise subtracted from our latents.</p>
<p>Finally, we pass the predicted noise and the current latents to our scheduler to prepare the latents for the next iteration(line <code>334</code>).</p>
<p>A new feature that was added recently is the callback function. If we pass any arguments to the <code>callback</code> argument, it gets executed inside the pipeline at line <code>338</code>. For example, let&rsquo;s create a simple function to pass into <code>callback</code> argument:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>sample_function</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>latents</span><span class=p>):</span>
    <span class=c1># callback function should expect the above 3 arguments</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</code></pre></div><p>Now if we pass this function to the pipeline as <code>pipe(prompt, callback=sample_function, callback_steps=3)</code>, then the function will be executed 3 times and each time it will print the values of <code>i</code> and <code>t</code>.</p>
<p>At last, we&rsquo;ve reached the end of the pipeline source code. This is where the latents are coverted to image format using the decoder part of VAE model and finally returned as a PIL image, as shown in the highlighted part below:</p>
<p><img loading=lazy src=/getting_started_stable_diffusion/sd-vae-decode-code.png alt="Code for decoding the latents">
</p>
<p>The code that is in between the highlighted part(line <code>348</code> to <code>356</code>) checks the generated image for NSFW(Not Safe For Work) content. If this type of content is detected, the pipeline returns a black image(as of the current version of the library - <code>v0.6.0</code>).</p>
<h2 id=concluding-thoughts>Concluding thoughts<a hidden class=anchor aria-hidden=true href=#concluding-thoughts>#</a></h2>
<p>This is just an overview of what&rsquo;s happening inside diffusion models and the stable diffusion pipeline in diffusers.</p>
<p>The FastAI course lectures and accompanying notebooks listed at the top of this blog post has some great explanations on the topics discussed so far. The notebooks also contain code to do some cool tricks using stable diffusion.</p>
<p>If you have any queries or want to discuss something machine learning, you could reach out to me via twitter <a href=https://twitter.com/@bkrish_>@bkrish_</a>.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://bipinkrishnan.github.io/tags/stable-diffusion/>stable-diffusion</a></li>
<li><a href=https://bipinkrishnan.github.io/tags/fastai-course/>fastai-course</a></li>
<li><a href=https://bipinkrishnan.github.io/tags/long-post/>Long posts</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://bipinkrishnan.github.io/posts/quick-explainer-activation-checkpointing/>
<span class=title>Â« Prev</span>
<br>
<span>Quick Explainer: Activation Checkpointing</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on twitter" href="https://twitter.com/intent/tweet/?text=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f&hashtags=stable-diffusion%2cfastai-course%2clong-post"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f&title=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&summary=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&source=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f&title=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on whatsapp" href="https://api.whatsapp.com/send?text=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion%20-%20https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Getting Started in the World of Stable Diffusion on telegram" href="https://telegram.me/share/url?text=Getting%20Started%20in%20the%20World%20of%20Stable%20Diffusion&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fgetting-started-in-the-world-of-stable-diffusion%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2025 <a href=https://bipinkrishnan.github.io>ðŸ“ŒPinnedNotes</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='copy';function d(){a.innerHTML='copied!',setTimeout(()=>{a.innerHTML='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>