<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Quick Explainer: Activation Checkpointing | üìåPinnedNotes</title>
<meta name=keywords content="quick-explainer,llm-scaling">
<meta name=description content="What is activation checkpointing? Activation checkpointing is a technique used to save GPU memory while training large deep learning models. During the training of a deep learning model, we store the activations in memory to calculate the gradients during the backward pass. Activation checkpointing literally skips the saving part, thus saving a lot of memory. The figure below will give you an idea of the huge amount of memory consumed by activations while training a model:">
<meta name=author content="Bipin Krishnan">
<link rel=canonical href=https://bipinkrishnan.github.io/posts/quick-explainer-activation-checkpointing/>
<link crossorigin=anonymous href=/assets/css/stylesheet.80f99cabbb3de64440a55e13c3af93619eaf6e0c2d7f8662f08e6b314c5a1e46.css integrity="sha256-gPmcq7s95kRApV4Tw6+TYZ6vbgwtf4Zi8I5rMUxaHkY=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://bipinkrishnan.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://bipinkrishnan.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://bipinkrishnan.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://bipinkrishnan.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://bipinkrishnan.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script>
<meta property="og:title" content="Quick Explainer: Activation Checkpointing">
<meta property="og:description" content="What is activation checkpointing? Activation checkpointing is a technique used to save GPU memory while training large deep learning models. During the training of a deep learning model, we store the activations in memory to calculate the gradients during the backward pass. Activation checkpointing literally skips the saving part, thus saving a lot of memory. The figure below will give you an idea of the huge amount of memory consumed by activations while training a model:">
<meta property="og:type" content="article">
<meta property="og:url" content="https://bipinkrishnan.github.io/posts/quick-explainer-activation-checkpointing/"><meta property="og:image" content="https://bipinkrishnan.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2025-02-23T20:12:50+00:00">
<meta property="article:modified_time" content="2025-02-23T20:12:50+00:00"><meta property="og:site_name" content="üìåPinnedNotes">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://bipinkrishnan.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Quick Explainer: Activation Checkpointing">
<meta name=twitter:description content="What is activation checkpointing? Activation checkpointing is a technique used to save GPU memory while training large deep learning models. During the training of a deep learning model, we store the activations in memory to calculate the gradients during the backward pass. Activation checkpointing literally skips the saving part, thus saving a lot of memory. The figure below will give you an idea of the huge amount of memory consumed by activations while training a model:">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://bipinkrishnan.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Quick Explainer: Activation Checkpointing","item":"https://bipinkrishnan.github.io/posts/quick-explainer-activation-checkpointing/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Quick Explainer: Activation Checkpointing","name":"Quick Explainer: Activation Checkpointing","description":"What is activation checkpointing? Activation checkpointing is a technique used to save GPU memory while training large deep learning models. During the training of a deep learning model, we store the activations in memory to calculate the gradients during the backward pass. Activation checkpointing literally skips the saving part, thus saving a lot of memory. The figure below will give you an idea of the huge amount of memory consumed by activations while training a model:","keywords":["quick-explainer","llm-scaling"],"articleBody":"What is activation checkpointing? Activation checkpointing is a technique used to save GPU memory while training large deep learning models. During the training of a deep learning model, we store the activations in memory to calculate the gradients during the backward pass. Activation checkpointing literally skips the saving part, thus saving a lot of memory. The figure below will give you an idea of the huge amount of memory consumed by activations while training a model:\nSource: The Ultra-Scale Playbook - Training LLMs on GPU Clusters\nSo instead of storing activations in memory, we re-calculate those activations during the backward pass. Thus, we essentially save a lot of memory at the cost of slightly more computation. Mostly we do selective activation checkpointing. This means we skip the storing of activations for some layers while keeping the rest of the activations in memory.\nHow are activations used in the backward pass? Here is a simple example of what we actually mean by activations and how they are used in the backward pass:\n  Consider a 3 layer neural network with $f_1$, $f_2$ and $f_3$ representing the transformation applied on each layer respectively.\n  Consider the input $x$, we get the final output of the neural network $y$ by literally doing this:\n$$ a_1 = f_1(x) $$ $$ a_2 = f_2(a_1) $$ $$ y = f_3(a_2 )$$\nwhere $a_1$, $a_2$ and $y$ are the activations of each layer.\n  Assume that the loss $L$ is calculated as follows:\n$$ L = (y - y_{true})^2 $$\nwhere $y_{true}$ is the ground truth.\n  To update the weights during the gradient descent step, we need to calculate the gradients for each layer using the chain rule. Let‚Äôs represent the gradients of each layer by $g_1$, $g_2$ and $g_3$ respectively. Here is a break down of the steps involved in the calculation of gradients for each layer:\n$$ g_3 = \\frac{\\partial L}{\\partial a_2} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial a_2} $$\n$$ g_2 = \\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial a_1} = g_3 \\cdot \\frac{\\partial a_2}{\\partial a_1} $$\n$$ g_1 = \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial x} = g_2 \\cdot \\frac{\\partial a_1}{\\partial x} $$\n  To calculate the gradient for a layer, we need the inputs and activations of that layer and the gradients of all the future layers. This is how we implement the above equations in code:\nimport torch ### Function transformation for each layer # f‚ÇÅ, f‚ÇÇ and f‚ÇÉ def f1(x): return x ** 2 def f2(x): return x ** 3 def f3(x): return x ** 4 # set `y_true=10` for now def loss(y, y_true=10): return (y - y_true) ** 2 ### Derivatives of each function transformation def df1(x): return 2 * x # ‚àÇL/‚àÇx def df2(x): return 3 * (x ** 2) # ‚àÇL/‚àÇa‚ÇÅ def df3(x): return 4 * (x ** 3) # ‚àÇL/‚àÇa‚ÇÇ def dloss(y, y_true=10): return 2 * (y - y_true) # ‚àÇL/‚àÇy ### Calculate the activations - forward pass x = torch.tensor(5., requires_grad=True) a1 = f1(x) # x ‚Üí f‚ÇÅ ‚Üí a‚ÇÅ a2 = f2(a1) # a‚ÇÅ ‚Üí f‚ÇÇ ‚Üí a‚ÇÇ y = f3(a2) # a‚ÇÇ ‚Üí f‚ÇÉ ‚Üí y l = loss(y) ### Calculate the gradients - backward pass # ‚àÇL/‚àÇa‚ÇÇ = (‚àÇL/‚àÇy).(‚àÇy/‚àÇa‚ÇÇ) g3 = dloss(y) * df3(a2) # ‚àÇL/‚àÇa‚ÇÅ = g‚ÇÉ.(‚àÇa‚ÇÇ/‚àÇa‚ÇÅ) g2 = g3 * df2(a1) # ‚àÇL/‚àÇx = g‚ÇÇ.(‚àÇaÔºë/‚àÇx) g1 = g2 * df1(x) You can compare the value of g1 with that of torch as follows:\n# calculate `‚àÇL/‚àÇx` using torch torch_grad, = torch.autograd.grad(l, x) # check if the gradients are equal torch.allclose(torch_grad, g1) ## Output: `True` Activation checkpointing in action PyTorch‚Äôs built-in activation checkpointing So the action plan here is to create a simple neural network and apply torch.utils.checkpoint to some layers in the model (selective activation checkpointing). We then compare the GPU memory with and without the activation checkpoint.\nThe neural network architecture (create utils.py file and save the below code):\nfrom torch import nn from torch.utils import checkpoint class MLP(nn.Module): def __init__(self, use_checkpoint=False): super().__init__() # whether to apply activation checkpointing self.use_checkpoint = use_checkpoint # X = block_1 - block_2 - block_3 = y self.block_1 = nn.Sequential( nn.Linear(3000, 512), nn.ReLU(), nn.Linear(512, 700), ) self.block_2 = nn.Sequential( nn.Linear(700, 512), nn.ReLU(), nn.Linear(512, 700), ) self.block_3 = nn.Sequential( nn.Linear(700, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): if self.use_checkpoint: # activations are not stored in memory for layers inside # `_checkpoint_layers` method, we recomputes it in the backward pass out = checkpoint.checkpoint(self._checkpoint_layers, x) else: out = self._checkpoint_layers(x) out = self.block_3(out) return out def _checkpoint_layers(self, x): return self.block_2(self.block_1(x)) For benchmarking the memory consumption, we will define a simple utility function get_mem_consumption (save this in utils.py). The function takes the model and passes in a random input and calculates the peak memory consumption in the GPU during the forward pass:\nimport torch def get_mem_consumption(model, device): # reset stats torch.cuda.reset_peak_memory_stats(device) # forward pass x = torch.randn(1024*3, 3000, device=device, requires_grad=True) out = model(x) # max memory consumption (in MB) max_mem = torch.cuda.max_memory_allocated(device) / 1e+6 # backward pass out.backward(torch.ones_like(out)) return max_mem Let‚Äôs also write a function (also saved in utils.py) to ease out the benchmarking process:\ndef run_benchmark(use_checkpoint): device = \"cuda\" # get the model model = MLP(use_checkpoint=use_checkpoint).to(device) # forward \u0026 backward pass, return memory consumption mem_consumption = get_mem_consumption(model, device) print(f\"Memory consumption with `use_checkpoint={use_checkpoint}`: {mem_consumption:.2f}MB\") We can simply call run_benchmark with use_checkpoint=True and use_checkpoint=False from the same script, but I found some discrepancies in the reported memory consumption while doing so. So I prefer to run them separately.\n GPU memory consumption without activation checkpointing:  from utils import run_benchmark run_benchmark(use_checkpoint=False) ## Output - Memory consumption with `use_checkpoint=False`: 101.04 MB GPU memory consumption with activation checkpointing:  from utils import run_benchmark run_benchmark(use_checkpoint=True) ## Output - Memory consumption with `use_checkpoint=True`: 83.21 MB Verify activation checkpointing with PyTorch hooks If you are like me, you might want to know if the activations are really getting stored in memory or not. We will write a simple PyTorch hook to verify this (saved in utils.py). We will apply this hook to each layer during the forward pass of the model.\ndef register_forward_hooks(model): # function called during the forward pass def forward_hook(module, inp, out): # for layers where `requires_grad=False`, the activations are  # re-computed during the backward pass print(f\"Forward pass for `{module.__class__.__name__}`: Activations stored = {out.requires_grad}\") # register forward hook for each layer in the model for name, layer in model.named_modules(): if isinstance(layer, nn.Sequential): for i in range(len(layer)): layer[i].register_forward_hook(forward_hook) Now we have to slightly modify run_benchmark function to incorporate this hook in the model before doing the forward pass:\ndef run_benchmark(use_checkpoint, verify=False): device = \"cuda\" # get the model model = MLP(use_checkpoint=use_checkpoint).to(device) if verify: # register hook register_forward_hooks(model) # forward \u0026 backward pass, return memory consumption mem_consumption = get_mem_consumption(model, device) print(f\"Memory consumption with `use_checkpoint={use_checkpoint}`: {mem_consumption:.2f}MB\") The output of running run_benchmark(use_checkpoint=True, verify=True) is as follows:\nForward pass for `Linear`: Activations stored = False Forward pass for `ReLU`: Activations stored = False Forward pass for `Linear`: Activations stored = False Forward pass for `Linear`: Activations stored = False Forward pass for `ReLU`: Activations stored = False Forward pass for `Linear`: Activations stored = False Forward pass for `Linear`: Activations stored = True Forward pass for `ReLU`: Activations stored = True Forward pass for `Linear`: Activations stored = True Forward pass for `Linear`: Activations stored = True Forward pass for `ReLU`: Activations stored = True Forward pass for `Linear`: Activations stored = True Forward pass for `Linear`: Activations stored = True Forward pass for `ReLU`: Activations stored = True Forward pass for `Linear`: Activations stored = True As expected, the activations for layers in the first two blocks of our neural network are not stored in memory during the forward pass.\nImplement your own activation checkpointing Finally, we are ready to implement our activation checkpointing class that we could use instead of torch.utils.checkpoint (implemented in utils.py). Here, sub-classing from torch.autograd.Function makes our lives easier. We could simply define the logic for modifying the normal behavior of forward and backward pass under forward and backward methods (pytorch team has a good documentation here if you are interested):\nclass CustomCheckpointFunction(torch.autograd.Function): @staticmethod def forward(ctx, func, inputs): # save the inputs \u0026 function transformation for backward pass # `func` would be the `_checkpoint_layers` defined in our model ctx.save_for_backward(inputs) ctx.func = func # return the output of the layer with torch.no_grad(): return func(inputs) @staticmethod def backward(ctx, grad_outputs): # get the inputs saved from forward pass inputs, = ctx.saved_tensors # re-compute the activation for the layer # using the function transformation saved from forward pass with torch.enable_grad(): outputs = ctx.func(inputs) # compute the gradients for the layer return (None, *torch.autograd.grad(outputs, inputs, grad_outputs)) In the final step of backward method, doing torch.autograd.grad(outputs, inputs, grad_outputs) is the same as $g_2 \\cdot \\frac{\\partial a_1}{\\partial x}$ and $g_3 \\cdot \\frac{\\partial a_2}{\\partial a_1}$ in mathematical sense. We multiply the gradients of future layers with gradient of the current layer (we‚Äôve discussed this earlier in the post).\nWe could simply replace torch.utils.checkpoint with our custom checkpoint function and it would work exactly the same. So our model definition would look like this, just a slight change in the forward method:\nclass MLP(nn.Module): def __init__(self, use_checkpoint=False): ... def forward(self, x): if self.use_checkpoint: # replace with our custom checkpoint function # func - `self._checkpoint_layers` # inputs - `x` out = CustomCheckpointFunction.apply(self._checkpoint_layers, x) else: out = self._checkpoint_layers(x) out = self.block_3(out) return out def _checkpoint_layers(self, x): ... Whoa, that‚Äôs a whole lot for a quick explainer :) Started from the theory, all the way to understanding gradient computation to implementing pytorch hooks to verify the checkpointing and finally implementing our own activation checkpointing using PyTorch‚Äôs autograd function. That would wrap up this quick explainer.\nYou can access the complete code used in this post here.\nReferences [1] ‚ÄúThe Ultra-Scale Playbook - a Hugging Face Space by Nanotron.‚Äù. Huggingface.co. 2025.\n[2] ‚ÄúPyTorch: Defining New Autograd Functions ‚Äî PyTorch Tutorials 2.6.0+Cu124 Documentation‚Äù. Pytorch.org. 2024.\n[3] ‚ÄúPyTorch 101: Understanding Hooks | DigitalOcean.‚Äù. Digitalocean.com. 2025. ‚Äå\n‚Äå\n","wordCount":"1663","inLanguage":"en","datePublished":"2025-02-23T20:12:50Z","dateModified":"2025-02-23T20:12:50Z","author":{"@type":"Person","name":"Bipin Krishnan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://bipinkrishnan.github.io/posts/quick-explainer-activation-checkpointing/"},"publisher":{"@type":"Organization","name":"üìåPinnedNotes","logo":{"@type":"ImageObject","url":"https://bipinkrishnan.github.io/favicon.ico"}}}</script>
</head>
<body class=dark id=top>
<script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://bipinkrishnan.github.io accesskey=h title="üìåPinnedNotes (Alt + H)">üìåPinnedNotes</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://bipinkrishnan.github.io/tags/long-post/ title="Long Posts">
<span>Long Posts</span>
</a>
</li>
<li>
<a href=https://bipinkrishnan.github.io/tags/quick-explainer/ title=Explainers>
<span>Explainers</span>
</a>
</li>
<li>
<a href=https://bipinkrishnan.github.io/archives/ title=Archives>
<span>Archives</span>
</a>
</li>
<li>
<a href=https://bipinkrishnan.github.io/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://bipinkrishnan.github.io>Home</a>&nbsp;¬ª&nbsp;<a href=https://bipinkrishnan.github.io/posts/>Posts</a></div>
<h1 class=post-title>
Quick Explainer: Activation Checkpointing
</h1>
<div class=post-meta><span title="2025-02-23 20:12:50 +0000 UTC">February 23, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;Bipin Krishnan
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#what-is-activation-checkpointing>What is activation checkpointing?</a></li>
<li><a href=#how-are-activations-used-in-the-backward-pass>How are activations used in the backward pass?</a></li>
<li><a href=#activation-checkpointing-in-action>Activation checkpointing in action</a>
<ul>
<li><a href=#pytorchs-built-in-activation-checkpointing>PyTorch&rsquo;s built-in activation checkpointing</a></li>
<li><a href=#verify-activation-checkpointing-with-pytorch-hooks>Verify activation checkpointing with PyTorch hooks</a></li>
<li><a href=#implement-your-own-activation-checkpointing>Implement your own activation checkpointing</a></li>
</ul>
</li>
<li><a href=#references>References</a></li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><h2 id=what-is-activation-checkpointing>What is activation checkpointing?<a hidden class=anchor aria-hidden=true href=#what-is-activation-checkpointing>#</a></h2>
<p>Activation checkpointing is a technique used to save GPU memory while training large deep learning models. During the training of a deep learning model, we store the activations in memory to calculate the gradients during the backward pass. Activation checkpointing literally skips the saving part, thus saving a lot of memory. The figure below will give you an idea of the huge amount of memory consumed by activations while training a model:</p>
<p><img loading=lazy src=/quick_explainers/activation_checkpointing/memory_comp_chart.png alt="Activation checkpointing">
<em>Source: The Ultra-Scale Playbook -
Training LLMs on GPU Clusters</em></p>
<p>So instead of storing activations in memory, we re-calculate those activations during the backward pass. Thus, we essentially save a lot of memory at the cost of slightly more computation. Mostly we do selective activation checkpointing. This means we skip the storing of activations for some layers while keeping the rest of the activations in memory.</p>
<h2 id=how-are-activations-used-in-the-backward-pass>How are activations used in the backward pass?<a hidden class=anchor aria-hidden=true href=#how-are-activations-used-in-the-backward-pass>#</a></h2>
<p>Here is a simple example of what we actually mean by activations and how they are used in the backward pass:</p>
<ul>
<li>
<p>Consider a 3 layer neural network with $f_1$, $f_2$ and $f_3$ representing the transformation applied on each layer respectively.</p>
</li>
<li>
<p>Consider the input $x$, we get the final output of the neural network $y$ by literally doing this:</p>
<p>$$ a_1 = f_1(x) $$
$$ a_2 = f_2(a_1) $$
$$ y = f_3(a_2 )$$</p>
<p>where $a_1$, $a_2$ and $y$ are the activations of each layer.</p>
</li>
<li>
<p>Assume that the loss $L$ is calculated as follows:</p>
<p>$$ L = (y - y_{true})^2 $$</p>
<p>where $y_{true}$ is the ground truth.</p>
</li>
<li>
<p>To update the weights during the gradient descent step, we need to calculate the gradients for each layer using the chain rule. Let&rsquo;s represent the gradients of each layer by $g_1$, $g_2$ and $g_3$ respectively. Here is a break down of the steps involved in the calculation of gradients for each layer:</p>
<p>$$ g_3 = \frac{\partial L}{\partial a_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a_2} $$</p>
<p>$$ g_2 = \frac{\partial L}{\partial a_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a_2} \cdot \frac{\partial a_2}{\partial a_1} = g_3 \cdot \frac{\partial a_2}{\partial a_1} $$</p>
<p>$$ g_1 = \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a_2} \cdot \frac{\partial a_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial x} = g_2 \cdot \frac{\partial a_1}{\partial x} $$</p>
</li>
</ul>
<p>To calculate the gradient for a layer, we need the inputs and activations of that layer and the gradients of all the future layers. This is how we implement the above equations in code:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>

<span class=c1>### Function transformation for each layer</span>

<span class=c1># f‚ÇÅ, f‚ÇÇ and f‚ÇÉ</span>
<span class=k>def</span> <span class=nf>f1</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>2</span>
<span class=k>def</span> <span class=nf>f2</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>3</span>
<span class=k>def</span> <span class=nf>f3</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=n>x</span> <span class=o>**</span> <span class=mi>4</span>

<span class=c1># set `y_true=10` for now</span>
<span class=k>def</span> <span class=nf>loss</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_true</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span> <span class=k>return</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span>

<span class=c1>### Derivatives of each function transformation</span>

<span class=k>def</span> <span class=nf>df1</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>x</span>                          <span class=c1># ‚àÇL/‚àÇx</span>
<span class=k>def</span> <span class=nf>df2</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=mi>3</span> <span class=o>*</span> <span class=p>(</span><span class=n>x</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>                   <span class=c1># ‚àÇL/‚àÇa‚ÇÅ</span>
<span class=k>def</span> <span class=nf>df3</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=mi>4</span> <span class=o>*</span> <span class=p>(</span><span class=n>x</span> <span class=o>**</span> <span class=mi>3</span><span class=p>)</span>                   <span class=c1># ‚àÇL/‚àÇa‚ÇÇ</span>
<span class=k>def</span> <span class=nf>dloss</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_true</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span> <span class=k>return</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>)</span>  <span class=c1># ‚àÇL/‚àÇy</span>

<span class=c1>### Calculate the activations -&gt; forward pass</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>5.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>a1</span> <span class=o>=</span> <span class=n>f1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>   <span class=c1># x ‚Üí f‚ÇÅ ‚Üí a‚ÇÅ</span>
<span class=n>a2</span> <span class=o>=</span> <span class=n>f2</span><span class=p>(</span><span class=n>a1</span><span class=p>)</span>  <span class=c1># a‚ÇÅ ‚Üí f‚ÇÇ ‚Üí a‚ÇÇ</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>f3</span><span class=p>(</span><span class=n>a2</span><span class=p>)</span>   <span class=c1># a‚ÇÇ ‚Üí f‚ÇÉ ‚Üí y</span>
<span class=n>l</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>

<span class=c1>### Calculate the gradients -&gt; backward pass</span>

<span class=c1># ‚àÇL/‚àÇa‚ÇÇ = (‚àÇL/‚àÇy).(‚àÇy/‚àÇa‚ÇÇ)</span>
<span class=n>g3</span> <span class=o>=</span> <span class=n>dloss</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>*</span> <span class=n>df3</span><span class=p>(</span><span class=n>a2</span><span class=p>)</span>

<span class=c1># ‚àÇL/‚àÇa‚ÇÅ = g‚ÇÉ.(‚àÇa‚ÇÇ/‚àÇa‚ÇÅ)</span>
<span class=n>g2</span> <span class=o>=</span> <span class=n>g3</span> <span class=o>*</span> <span class=n>df2</span><span class=p>(</span><span class=n>a1</span><span class=p>)</span>

<span class=c1># ‚àÇL/‚àÇx = g‚ÇÇ.(‚àÇaÔºë/‚àÇx)</span>
<span class=n>g1</span> <span class=o>=</span> <span class=n>g2</span> <span class=o>*</span> <span class=n>df1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</code></pre></div><p>You can compare the value of <code>g1</code> with that of torch as follows:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=c1># calculate `‚àÇL/‚àÇx` using torch</span>
<span class=n>torch_grad</span><span class=p>,</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>grad</span><span class=p>(</span><span class=n>l</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>

<span class=c1># check if the gradients are equal</span>
<span class=n>torch</span><span class=o>.</span><span class=n>allclose</span><span class=p>(</span><span class=n>torch_grad</span><span class=p>,</span> <span class=n>g1</span><span class=p>)</span>
<span class=c1>## Output: `True`</span>
</code></pre></div><h2 id=activation-checkpointing-in-action>Activation checkpointing in action<a hidden class=anchor aria-hidden=true href=#activation-checkpointing-in-action>#</a></h2>
<h3 id=pytorchs-built-in-activation-checkpointing>PyTorch&rsquo;s built-in activation checkpointing<a hidden class=anchor aria-hidden=true href=#pytorchs-built-in-activation-checkpointing>#</a></h3>
<p>So the action plan here is to create a simple neural network and apply <code>torch.utils.checkpoint</code> to some layers in the model (selective activation checkpointing). We then compare the GPU memory with and without the activation checkpoint.</p>
<p>The neural network architecture (create <code>utils.py</code> file and save the below code):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
<span class=kn>from</span> <span class=nn>torch.utils</span> <span class=kn>import</span> <span class=n>checkpoint</span>


<span class=k>class</span> <span class=nc>MLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>use_checkpoint</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># whether to apply activation checkpointing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span> <span class=o>=</span> <span class=n>use_checkpoint</span>

        <span class=c1># X =&gt; block_1 -&gt; block_2 -&gt; block_3 =&gt; y</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>block_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>3000</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>700</span><span class=p>),</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>block_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>700</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>700</span><span class=p>),</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>block_3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>700</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
        <span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span><span class=p>:</span>
            <span class=c1># activations are not stored in memory for layers inside</span>
            <span class=c1># `_checkpoint_layers` method, we recomputes it in the backward pass</span>
            <span class=n>out</span> <span class=o>=</span> <span class=n>checkpoint</span><span class=o>.</span><span class=n>checkpoint</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_checkpoint_layers</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_checkpoint_layers</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block_3</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

    <span class=k>def</span> <span class=nf>_checkpoint_layers</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>block_2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>block_1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</code></pre></div><p>For benchmarking the memory consumption, we will define a simple utility function <code>get_mem_consumption</code> (save this in <code>utils.py</code>). The function takes the model and passes in a random input and calculates the peak memory consumption in the GPU during the forward pass:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>

<span class=k>def</span> <span class=nf>get_mem_consumption</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>):</span>
    <span class=c1># reset stats</span>
    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>reset_peak_memory_stats</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

    <span class=c1># forward pass</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1024</span><span class=o>*</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3000</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
    <span class=n>out</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

    <span class=c1># max memory consumption (in MB)</span>
    <span class=n>max_mem</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>max_memory_allocated</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1e+6</span>
    <span class=c1># backward pass</span>
    <span class=n>out</span><span class=o>.</span><span class=n>backward</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>out</span><span class=p>))</span>

    <span class=k>return</span> <span class=n>max_mem</span>
</code></pre></div><p>Let&rsquo;s also write a function (also saved in <code>utils.py</code>) to ease out the benchmarking process:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>run_benchmark</span><span class=p>(</span><span class=n>use_checkpoint</span><span class=p>):</span>
    <span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span>

    <span class=c1># get the model</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>use_checkpoint</span><span class=o>=</span><span class=n>use_checkpoint</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
    <span class=c1># forward &amp; backward pass, return memory consumption</span>
    <span class=n>mem_consumption</span> <span class=o>=</span> <span class=n>get_mem_consumption</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Memory consumption with `use_checkpoint=</span><span class=si>{</span><span class=n>use_checkpoint</span><span class=si>}</span><span class=s2>`: </span><span class=si>{</span><span class=n>mem_consumption</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB&#34;</span><span class=p>)</span>
</code></pre></div><p>We can simply call <code>run_benchmark</code> with <code>use_checkpoint=True</code> and <code>use_checkpoint=False</code> from the same script, but I found some discrepancies in the reported memory consumption while doing so. So I prefer to run them separately.</p>
<ol>
<li>GPU memory consumption <strong>without</strong> activation checkpointing:</li>
</ol>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>utils</span> <span class=kn>import</span> <span class=n>run_benchmark</span>

<span class=n>run_benchmark</span><span class=p>(</span><span class=n>use_checkpoint</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=c1>## Output - Memory consumption with `use_checkpoint=False`: 101.04 MB</span>
</code></pre></div><ol start=2>
<li>GPU memory consumption <strong>with</strong> activation checkpointing:</li>
</ol>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>utils</span> <span class=kn>import</span> <span class=n>run_benchmark</span>

<span class=n>run_benchmark</span><span class=p>(</span><span class=n>use_checkpoint</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=c1>## Output - Memory consumption with `use_checkpoint=True`: 83.21 MB</span>
</code></pre></div><h3 id=verify-activation-checkpointing-with-pytorch-hooks>Verify activation checkpointing with PyTorch hooks<a hidden class=anchor aria-hidden=true href=#verify-activation-checkpointing-with-pytorch-hooks>#</a></h3>
<p>If you are like me, you might want to know if the activations are really getting stored in memory or not. We will write a simple PyTorch hook to verify this (saved in <code>utils.py</code>). We will apply this hook to each layer during the forward pass of the model.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>register_forward_hooks</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>

    <span class=c1># function called during the forward pass</span>
    <span class=k>def</span> <span class=nf>forward_hook</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>inp</span><span class=p>,</span> <span class=n>out</span><span class=p>):</span>
        <span class=c1># for layers where `requires_grad=False`, the activations are </span>
        <span class=c1># re-computed during the backward pass</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Forward pass for `</span><span class=si>{</span><span class=n>module</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span><span class=si>}</span><span class=s2>`: Activations stored = </span><span class=si>{</span><span class=n>out</span><span class=o>.</span><span class=n>requires_grad</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>

    <span class=c1># register forward hook for each layer in the model</span>
    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>():</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>):</span>
            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>layer</span><span class=p>)):</span>
                <span class=n>layer</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>register_forward_hook</span><span class=p>(</span><span class=n>forward_hook</span><span class=p>)</span>
</code></pre></div><p>Now we have to slightly modify <code>run_benchmark</code> function to incorporate this hook in the model before doing the forward pass:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>run_benchmark</span><span class=p>(</span><span class=n>use_checkpoint</span><span class=p>,</span> <span class=n>verify</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
    <span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span>

    <span class=c1># get the model</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>use_checkpoint</span><span class=o>=</span><span class=n>use_checkpoint</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>verify</span><span class=p>:</span>
        <span class=c1># register hook</span>
        <span class=n>register_forward_hooks</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>

    <span class=c1># forward &amp; backward pass, return memory consumption</span>
    <span class=n>mem_consumption</span> <span class=o>=</span> <span class=n>get_mem_consumption</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Memory consumption with `use_checkpoint=</span><span class=si>{</span><span class=n>use_checkpoint</span><span class=si>}</span><span class=s2>`: </span><span class=si>{</span><span class=n>mem_consumption</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB&#34;</span><span class=p>)</span>
</code></pre></div><p>The output of running <code>run_benchmark(use_checkpoint=True, verify=True)</code> is as follows:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>False</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>ReLU</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>False</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>False</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>False</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>ReLU</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>False</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>False</span>

<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>ReLU</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>ReLU</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>ReLU</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>Forward</span> <span class=k>pass</span> <span class=k>for</span> <span class=err>`</span><span class=n>Linear</span><span class=err>`</span><span class=p>:</span> <span class=n>Activations</span> <span class=n>stored</span> <span class=o>=</span> <span class=kc>True</span>
</code></pre></div><p>As expected, the activations for layers in the first two blocks of our neural network are not stored in memory during the forward pass.</p>
<h3 id=implement-your-own-activation-checkpointing>Implement your own activation checkpointing<a hidden class=anchor aria-hidden=true href=#implement-your-own-activation-checkpointing>#</a></h3>
<p>Finally, we are ready to implement our activation checkpointing class that we could use instead of <code>torch.utils.checkpoint</code> (implemented in <code>utils.py</code>). Here, sub-classing from <code>torch.autograd.Function</code> makes our lives easier. We could simply define the logic for modifying the normal behavior of forward and backward pass under <code>forward</code> and <code>backward</code> methods (pytorch team has a good documentation <a href=https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html>here</a> if you are interested):</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>CustomCheckpointFunction</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>Function</span><span class=p>):</span>
    <span class=nd>@staticmethod</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>func</span><span class=p>,</span> <span class=n>inputs</span><span class=p>):</span>
        <span class=c1># save the inputs &amp; function transformation for backward pass</span>
        <span class=c1># `func` would be the `_checkpoint_layers` defined in our model</span>
        <span class=n>ctx</span><span class=o>.</span><span class=n>save_for_backward</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
        <span class=n>ctx</span><span class=o>.</span><span class=n>func</span> <span class=o>=</span> <span class=n>func</span>

        <span class=c1># return the output of the layer</span>
        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
            <span class=k>return</span> <span class=n>func</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>

    <span class=nd>@staticmethod</span>
    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>grad_outputs</span><span class=p>):</span>
        <span class=c1># get the inputs saved from forward pass</span>
        <span class=n>inputs</span><span class=p>,</span> <span class=o>=</span> <span class=n>ctx</span><span class=o>.</span><span class=n>saved_tensors</span>
        <span class=c1># re-compute the activation for the layer</span>
        <span class=c1># using the function transformation saved from forward pass</span>
        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>enable_grad</span><span class=p>():</span>
            <span class=n>outputs</span> <span class=o>=</span> <span class=n>ctx</span><span class=o>.</span><span class=n>func</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>

        <span class=c1># compute the gradients for the layer</span>
        <span class=k>return</span> <span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=o>*</span><span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>grad</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>grad_outputs</span><span class=p>))</span>
</code></pre></div><p>In the final step of <code>backward</code> method, doing <code>torch.autograd.grad(outputs, inputs, grad_outputs)</code> is the same as $g_2 \cdot \frac{\partial a_1}{\partial x}$ and $g_3 \cdot \frac{\partial a_2}{\partial a_1}$ in mathematical sense. We multiply the gradients of future layers with gradient of the current layer (we&rsquo;ve discussed this earlier in the post).</p>
<p>We could simply replace <code>torch.utils.checkpoint</code> with our custom checkpoint function and it would work exactly the same. So our model definition would look like this, just a slight change in the <code>forward</code> method:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>MLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>use_checkpoint</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
        <span class=o>...</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span><span class=p>:</span>
            <span class=c1># replace with our custom checkpoint function</span>
            <span class=c1># func -&gt; `self._checkpoint_layers`</span>
            <span class=c1># inputs -&gt; `x`</span>
            <span class=n>out</span> <span class=o>=</span> <span class=n>CustomCheckpointFunction</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_checkpoint_layers</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_checkpoint_layers</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block_3</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>out</span>

    <span class=k>def</span> <span class=nf>_checkpoint_layers</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=o>...</span>
</code></pre></div><p>Whoa, that&rsquo;s a whole lot for a quick explainer :) Started from the theory, all the way to understanding gradient computation to implementing pytorch hooks to verify the checkpointing and finally implementing our own activation checkpointing using PyTorch&rsquo;s autograd function. That would wrap up this quick explainer.</p>
<p>You can access the complete code used in this post <a href=https://github.com/bipinKrishnan/ml_engineering/tree/main/activation_checkpointing>here</a>.</p>
<h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2>
<p>[1] ‚Äú<a href=https://huggingface.co/spaces/nanotron/ultrascale-playbook>The Ultra-Scale Playbook - a Hugging Face Space by Nanotron.</a>‚Äù. Huggingface.co. 2025.</p>
<p>[2] ‚Äú<a href=https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html>PyTorch: Defining New Autograd Functions ‚Äî PyTorch Tutorials 2.6.0+Cu124 Documentation</a>‚Äù. Pytorch.org. 2024.</p>
<p>[3] ‚Äú<a href=https://www.digitalocean.com/community/tutorials/pytorch-hooks-gradient-clipping-debugging>PyTorch 101: Understanding Hooks | DigitalOcean.</a>‚Äù. Digitalocean.com. 2025.
‚Äå</p>
<p>‚Äå</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://bipinkrishnan.github.io/tags/quick-explainer/>Quick Explainers</a></li>
<li><a href=https://bipinkrishnan.github.io/tags/llm-scaling/>llm-scaling</a></li>
</ul>
<nav class=paginav>
<a class=next href=https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/>
<span class=title>Next ¬ª</span>
<br>
<span>Getting Started in the World of Stable Diffusion</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Quick Explainer: Activation Checkpointing on twitter" href="https://twitter.com/intent/tweet/?text=Quick%20Explainer%3a%20Activation%20Checkpointing&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f&hashtags=quick-explainer%2cllm-scaling"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Quick Explainer: Activation Checkpointing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f&title=Quick%20Explainer%3a%20Activation%20Checkpointing&summary=Quick%20Explainer%3a%20Activation%20Checkpointing&source=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Quick Explainer: Activation Checkpointing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f&title=Quick%20Explainer%3a%20Activation%20Checkpointing"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Quick Explainer: Activation Checkpointing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Quick Explainer: Activation Checkpointing on whatsapp" href="https://api.whatsapp.com/send?text=Quick%20Explainer%3a%20Activation%20Checkpointing%20-%20https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Quick Explainer: Activation Checkpointing on telegram" href="https://telegram.me/share/url?text=Quick%20Explainer%3a%20Activation%20Checkpointing&url=https%3a%2f%2fbipinkrishnan.github.io%2fposts%2fquick-explainer-activation-checkpointing%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2025 <a href=https://bipinkrishnan.github.io>üìåPinnedNotes</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='copy';function d(){a.innerHTML='copied!',setTimeout(()=>{a.innerHTML='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>